{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manikethaley/Documents/Fetch_TakeHomeTest/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check whether the Bert model able to load properly in the machine or not\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name, force_download=True)\n",
    "    model = TFBertModel.from_pretrained(model_name, force_download=True)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: tf.Tensor(\n",
      "[[-0.2708747  -0.44847974  0.18935685 ... -0.2094822   0.13687545\n",
      "  -0.06472529]\n",
      " [ 0.04127148 -0.33472702  0.3767434  ... -0.45713213 -0.24520482\n",
      "  -0.03398323]], shape=(2, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel, BertTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, force_download=True)\n",
    "model = TFBertModel.from_pretrained(model_name, force_download=True)\n",
    "\n",
    "# Sample sentences for embedding generation\n",
    "sentences = [\"This is a test sentence.\", \"Transformers are powerful models for NLP tasks.\"]\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs = tokenizer(sentences, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Generate embeddings using the model\n",
    "outputs = model(inputs.input_ids)\n",
    "\n",
    "# Mean pooling to get a fixed-size vector for each sentence\n",
    "embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
    "\n",
    "print(\"Embeddings:\", embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-1 Explanation\n",
    "\n",
    "In this task, the model architecture is primarily based on the BERT transformer backbone (TFBertModel). There were no significant changes or additional architectural choices beyond using the pre-trained BERT model and generating sentence embeddings. However, here are some considerations made during the design:\n",
    "\n",
    "1. Pooling Strategy:\n",
    "\n",
    "The embeddings are generated by applying mean pooling over the token embeddings (outputs.last_hidden_state) to obtain a fixed-size vector for each sentence. This is a choice to summarize the sentence while keeping it independent of sentence length.\n",
    "\n",
    "2. Tokenization and Padding:\n",
    "\n",
    "Padding and truncation are used to ensure all sentences are of the same length (max_length=128). This is necessary for batch processing with fixed-size input vectors.\n",
    "\n",
    "3. Output Representation:\n",
    "\n",
    "The model's outputs are directly taken from the final hidden states of the BERT model. No additional layers or task-specific heads (such as classification layers) are added in this task, as the goal is just to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the sentence embeddings in your machine\n",
    "import numpy as np\n",
    "np.save(\"sentence_embeddings.npy\", embeddings.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2:Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Output: tf.Tensor(\n",
      "[[0.57701844 0.20382427 0.21915731]\n",
      " [0.48584175 0.11417866 0.39997956]], shape=(2, 3), dtype=float32)\n",
      "Sentiment Output: tf.Tensor(\n",
      "[[0.21390598 0.786094  ]\n",
      " [0.47390592 0.526094  ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Task-2:Multi-Task Learning Expansion\n",
    "\n",
    "class MultiTaskSentenceTransformer(tf.keras.Model):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=3):\n",
    "        super(MultiTaskSentenceTransformer, self).__init__()\n",
    "        \n",
    "        # Load the tokenizer and BERT model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.bert = TFBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Define task-specific output layers\n",
    "        self.classification_head = tf.keras.layers.Dense(num_classes, activation='softmax')  # For Sentence Classification\n",
    "        self.sentiment_head = tf.keras.layers.Dense(2, activation='softmax')  # Binary sentiment classification (Positive/Negative)\n",
    "\n",
    "    def call(self, sentences, task=\"classification\"):\n",
    "        # Tokenize and pass through BERT model\n",
    "        inputs = self.tokenizer(sentences, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
    "        outputs = self.bert(inputs.input_ids)\n",
    "        pooled_output = tf.reduce_mean(outputs.last_hidden_state, axis=1)  # Pooling output for each sentence\n",
    "        \n",
    "        # Select task-specific head\n",
    "        if task == \"classification\":\n",
    "            return self.classification_head(pooled_output)  # For Sentence Classification\n",
    "        elif task == \"sentiment\":\n",
    "            return self.sentiment_head(pooled_output)  # For Sentiment Analysis\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task specified. Choose between 'classification' and 'sentiment'.\")\n",
    "\n",
    "# Testing the multi-task model\n",
    "multi_task_model = MultiTaskSentenceTransformer()\n",
    "sentences = [\"This is a test sentence.\", \"Transformers are powerful models for NLP tasks.\"]\n",
    "\n",
    "classification_output = multi_task_model(sentences, task=\"classification\")\n",
    "sentiment_output = multi_task_model(sentences, task=\"sentiment\")\n",
    "\n",
    "print(\"Classification Output:\", classification_output)\n",
    "print(\"Sentiment Output:\", sentiment_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-2: Explanation\n",
    "### Describe the changes made to the architecture to support multi-task learning?\n",
    "\n",
    "1. **Shared BERT Encoder:** \n",
    "\n",
    "        The BERT model (TFBertModel) is used as a shared encoder to generate contextual embeddings for the input sentences, which are then used for both tasks.\n",
    "\n",
    "2. **Task-Specific Heads:**\n",
    "\n",
    "        A. Sentence Classification: A softmax layer is added on top of the embeddings for sentence classification.\n",
    "\n",
    "        B. Sentiment Analysis: A sigmoid layer is added for binary sentiment classification (positive/negative).\n",
    "\n",
    "3. **Multi-Task Setup:** \n",
    "        \n",
    "        Both heads share the same BERT encoder but have separate output layers.Each task has its own loss function (e.g., categorical cross-entropy for classification, binary cross-entropy for sentiment analysis).\n",
    "\n",
    "4. **Training:** The model is trained simultaneously on both tasks, optimizing for both losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Training Considerations\n",
    "1. **Freezing the Entire Network:**\n",
    "\n",
    "*Scenario:* In this case, the entire network (including the transformer backbone and both task-specific heads) is frozen. The model will not be updated during training, meaning no gradients will be computed for the weights in the network.\n",
    "\n",
    "**Implications and Advantages:**\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "A. **Pre-trained Features:** This is useful when you want to use the pre-trained model as a feature extractor without modifying it. The transformer layer captures rich linguistic features, so freezing the entire model can save computational resources.\n",
    "\n",
    "B. **Limited Data:** If the dataset is small and you don’t want to risk overfitting, freezing the model could be useful. You can still learn task-specific heads without requiring large-scale data for fine-tuning.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "**Limited Flexibility:** The model won’t adapt to your specific task, since the weights will remain unchanged. If the pre-trained model doesn’t generalize well to your specific data, the performance could be suboptimal.\n",
    "\n",
    "\n",
    "\n",
    "2.**Freezing the Transformer Backbone (BERT) and Unfreezing Task-Specific Heads**\n",
    "\n",
    "Scenario: In this case, the transformer backbone (BERT) is frozen, and only the task-specific heads (the classification and sentiment heads) are trainable. This allows the pre-trained BERT model to provide rich feature representations, while training only the final layers for the specific tasks.\n",
    "\n",
    "**Implications and Advantages:**\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "A. **Preserving Pre-trained Knowledge:** The transformer model (BERT) has been pre-trained on large datasets, which gives it a rich understanding of language. By freezing it, we preserve this knowledge and use it for our tasks without having to re-learn it from scratch.\n",
    "\n",
    "B. **Faster Training:** Training only the heads (classification and sentiment) reduces the computational load significantly, as the majority of the model is frozen.\n",
    "\n",
    "C. **Task-Specific Adaptation:** The heads will be fine-tuned to the specific tasks (e.g., classification and sentiment analysis), so they can learn the necessary mapping from BERT’s features to task-specific outputs.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "**Suboptimal Performance:** If the transformer’s representation is not optimal for your tasks, freezing it could limit the model’s ability to adapt and improve. In some cases, the transformer’s feature representation might need task-specific adjustments to achieve the best performance.\n",
    "\n",
    "\n",
    "\n",
    "3. **Freezing One of the Task-Specific Heads (Task A or Task B)**\n",
    "\n",
    "Scenario: In this scenario, we freeze one of the task-specific heads while leaving the other one trainable. For instance, you could freeze the classification head and only train the sentiment head, or vice versa.\n",
    "\n",
    "**Implications and Advantages:**\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "A. **Task-Specific Fine-tuning:** This is useful if you have more data for one task than the other. For example, if you have a large dataset for sentiment analysis but a smaller dataset for classification, you can fine-tune the sentiment head while keeping the classification head fixed.\n",
    "\n",
    "B. **Efficiency:** If one task (e.g., sentiment analysis) is more critical or needs more fine-tuning, this approach allows you to focus resources on that specific task while preserving the rest of the model.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "**Imbalance:** Freezing one of the heads means you won’t be able to jointly optimize both tasks. This could lead to suboptimal performance on the frozen task since it won’t be able to adapt further.\n",
    "\n",
    "\n",
    "\n",
    "### Transfer Learning: A Beneficial Scenario\n",
    "\n",
    "Transfer learning allows us to take a pre-trained model and adapt it to a new task. Here's how you can approach the transfer learning process with a BERT-based model, particularly in the context of multi-task learning:\n",
    "\n",
    "1. **Choice of Pre-Trained Model**\n",
    "\n",
    "The choice of a pre-trained model depends on the task you're solving. For this example, we are using BERT (bert-base-uncased), which is pre-trained on large corpora and has been shown to work well for a variety of NLP tasks.\n",
    "\n",
    "**Pre-trained Model:** bert-base-uncased is a commonly used version for transfer learning in NLP tasks. It has been trained on large text corpora and provides strong feature representations for sentences.\n",
    "\n",
    "2. **Which Layers to Freeze/Unfreeze**\n",
    "\n",
    "A. **Freeze the Transformer Backbone (BERT):**\n",
    "\n",
    "For fine-tuning on task-specific heads, we would generally freeze the BERT backbone, since it already captures useful language features. This allows us to adapt the final dense layers (task-specific heads) without changing the transformer’s weights.\n",
    "\n",
    "Why Freeze the Transformer? Freezing the transformer prevents overfitting and allows the model to focus on learning task-specific features. BERT is already well-trained on language modeling, so we don't need to retrain it.\n",
    "\n",
    "B. **Unfreeze the Task-Specific Heads:** \n",
    "\n",
    "These layers are trainable because we want to adapt the final decision-making layers to the specific tasks (sentence classification and sentiment analysis). Each task-specific head can learn to map the transformer’s output to the correct task outputs.\n",
    "\n",
    "3. **Rationale Behind Freezing/Unfreezing**\n",
    "\n",
    "A. **Freezing the Backbone:** \n",
    "\n",
    "BERT has already been pre-trained on a large corpus and learned to capture deep language features. Fine-tuning this on a small dataset might lead to overfitting or slower convergence, so freezing helps in faster adaptation to new tasks without losing generalization.\n",
    "\n",
    "B.**Unfreezing the Task-Specific Heads:** \n",
    "\n",
    "We want to learn task-specific mappings from the BERT features to the desired outputs. By unfreezing the heads, we allow the model to adapt to the specific nature of the tasks (classification and sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-4: Layer-wise Learning Rate Implementation (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-4:\n",
    "\n",
    "# Function to create the optimizer with layer-wise learning rates\n",
    "def get_optimizer(model, base_lr=1e-5):\n",
    "    # Initialize the Adam optimizer\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=base_lr)\n",
    "    \n",
    "    # Create a list of learning rates based on variable names\n",
    "    layer_lr = {}\n",
    "\n",
    "    for var in model.trainable_variables:\n",
    "        # Check if the variable name corresponds to BERT layers\n",
    "        if 'bert' in var.name:\n",
    "            # Use the count of 'layer' in the variable name to define the learning rate\n",
    "            layer_index = var.name.count('layer')\n",
    "            layer_lr[var.name] = base_lr * (0.9 ** layer_index)\n",
    "\n",
    "    # Define a custom learning rate schedule based on variable names\n",
    "    def lr_schedule(variable):\n",
    "        # Return the learning rate for the corresponding layer, or use base_lr if not found\n",
    "        return layer_lr.get(variable.name, base_lr)\n",
    "\n",
    "    # Create a custom optimizer that updates learning rate according to layer\n",
    "    class LayerWiseLearningRate(tf.keras.optimizers.legacy.Adam):\n",
    "        def apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True):\n",
    "            # Apply custom learning rate schedule to each gradient update\n",
    "            grads_and_vars = [(grad, var) for grad, var in grads_and_vars if grad is not None]\n",
    "            for i, (grad, var) in enumerate(grads_and_vars):\n",
    "                layer_lr_value = lr_schedule(var)\n",
    "                grads_and_vars[i] = (grad * layer_lr_value, var)\n",
    "            return super().apply_gradients(grads_and_vars, name, experimental_aggregate_gradients)\n",
    "\n",
    "    # Use the custom optimizer with layer-wise learning rate\n",
    "    return LayerWiseLearningRate()\n",
    "\n",
    "# Assuming multi_task_model is already created\n",
    "optimizer = get_optimizer(multi_task_model)\n",
    "\n",
    "# Compile the model with the new optimizer\n",
    "multi_task_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationale for Learning Rates:\n",
    "\n",
    "1. **Lower Layers:** These layers focus on general language features, so they need smaller learning rates to avoid drastic changes.\n",
    "\n",
    "2. **Upper Layers:** These layers are more task-specific and benefit from larger learning rates to adapt quickly.\n",
    "\n",
    "3. **Exponential Decay:** The learning rate decays exponentially as the layer index increases, balancing training stability and task adaptation.\n",
    "\n",
    "### Benefits of Layer-wise Learning Rates:\n",
    "\n",
    "1. **Efficient Training:** Lower layers adjust slowly to preserve pre-trained features, while upper layers learn quickly for task adaptation.\n",
    "\n",
    "2. **Stabilized Training:** Fine-tuning upper layers while keeping lower layers frozen prevents overfitting and stabilizes the training process.\n",
    "\n",
    "3. **Faster Convergence:** Larger learning rates for upper layers allow faster convergence, while smaller rates for lower layers prevent drastic changes.\n",
    "\n",
    "4. **Pre-trained Models:** BERT’s lower layers are already well-trained, so they don’t need significant updates during fine-tuning.\n",
    "\n",
    "### Effect in Multi-task Learning:\n",
    "\n",
    "In multi-task learning, layer-wise learning rates help balance task-specific fine-tuning while maintaining generalization across tasks:\n",
    "\n",
    "1. **Task-Specific Fine-Tuning:** Upper layers are tuned to each task, while lower layers are shared across tasks.\n",
    "\n",
    "2. **Avoiding Overfitting:** Conservative updates to shared layers prevent overfitting to one task while generalizing to others.\n",
    "\n",
    "3. **Task Interference:** Layer-wise rates prevent one task from dominating the learning process, ensuring efficient multitasking.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
